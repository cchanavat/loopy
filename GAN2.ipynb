{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoop(torch.utils.data.Dataset):\n",
    "    def __init__(self, ndim, nsamples, loop_type=\"raw\"):\n",
    "        self.ndim = ndim\n",
    "        self.nsamples = nsamples\n",
    "        self.loop_type = loop_type\n",
    "        self.dir = \"data/{0}/{0}_{1}/\".format(self.loop_type, self.ndim)\n",
    "        self.loops = {}\n",
    "        self.label = 0\n",
    "        \n",
    "        self.get_loops()\n",
    "        \n",
    "    def get_loops(self):\n",
    "        for i in range(self.nsamples):\n",
    "            try:\n",
    "                self.loops[i] = (np.loadtxt(self.dir + \"{}.loop\".format(i)).astype(\"int\"), self.label)\n",
    "            except OSError as e:\n",
    "                print(self.dir + \"{}.loop\".format(i) + \" loop not found.\")\n",
    "           \n",
    "    def __getitem__(self, i):\n",
    "        return self.loops[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetLoop(opt.img_size, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10] [Batch 0/79] [D loss: 0.626474] [G loss: 0.706296]\n",
      "[Epoch 0/10] [Batch 1/79] [D loss: 0.570631] [G loss: 0.705580]\n",
      "[Epoch 0/10] [Batch 2/79] [D loss: 0.522406] [G loss: 0.705029]\n",
      "[Epoch 0/10] [Batch 3/79] [D loss: 0.487363] [G loss: 0.704383]\n",
      "[Epoch 0/10] [Batch 4/79] [D loss: 0.460753] [G loss: 0.703659]\n",
      "[Epoch 0/10] [Batch 5/79] [D loss: 0.440937] [G loss: 0.702783]\n",
      "[Epoch 0/10] [Batch 6/79] [D loss: 0.419770] [G loss: 0.702260]\n",
      "[Epoch 0/10] [Batch 7/79] [D loss: 0.409598] [G loss: 0.700072]\n",
      "[Epoch 0/10] [Batch 8/79] [D loss: 0.399852] [G loss: 0.698955]\n",
      "[Epoch 0/10] [Batch 9/79] [D loss: 0.392688] [G loss: 0.697356]\n",
      "[Epoch 0/10] [Batch 10/79] [D loss: 0.388570] [G loss: 0.695473]\n",
      "[Epoch 0/10] [Batch 11/79] [D loss: 0.384205] [G loss: 0.692455]\n",
      "[Epoch 0/10] [Batch 12/79] [D loss: 0.381006] [G loss: 0.689505]\n",
      "[Epoch 0/10] [Batch 13/79] [D loss: 0.379109] [G loss: 0.686606]\n",
      "[Epoch 0/10] [Batch 14/79] [D loss: 0.377874] [G loss: 0.681837]\n",
      "[Epoch 0/10] [Batch 15/79] [D loss: 0.377762] [G loss: 0.679011]\n",
      "[Epoch 0/10] [Batch 16/79] [D loss: 0.379081] [G loss: 0.672383]\n",
      "[Epoch 0/10] [Batch 17/79] [D loss: 0.379647] [G loss: 0.666643]\n",
      "[Epoch 0/10] [Batch 18/79] [D loss: 0.384394] [G loss: 0.656165]\n",
      "[Epoch 0/10] [Batch 19/79] [D loss: 0.385864] [G loss: 0.651137]\n",
      "[Epoch 0/10] [Batch 20/79] [D loss: 0.390827] [G loss: 0.640887]\n",
      "[Epoch 0/10] [Batch 21/79] [D loss: 0.396871] [G loss: 0.629208]\n",
      "[Epoch 0/10] [Batch 22/79] [D loss: 0.407338] [G loss: 0.611438]\n",
      "[Epoch 0/10] [Batch 23/79] [D loss: 0.409873] [G loss: 0.606973]\n",
      "[Epoch 0/10] [Batch 24/79] [D loss: 0.416822] [G loss: 0.594839]\n",
      "[Epoch 0/10] [Batch 25/79] [D loss: 0.428983] [G loss: 0.577093]\n",
      "[Epoch 0/10] [Batch 26/79] [D loss: 0.431862] [G loss: 0.573576]\n",
      "[Epoch 0/10] [Batch 27/79] [D loss: 0.442454] [G loss: 0.559151]\n",
      "[Epoch 0/10] [Batch 28/79] [D loss: 0.454160] [G loss: 0.543543]\n",
      "[Epoch 0/10] [Batch 29/79] [D loss: 0.449443] [G loss: 0.552722]\n",
      "[Epoch 0/10] [Batch 30/79] [D loss: 0.454514] [G loss: 0.547817]\n",
      "[Epoch 0/10] [Batch 31/79] [D loss: 0.456747] [G loss: 0.546489]\n",
      "[Epoch 0/10] [Batch 32/79] [D loss: 0.448779] [G loss: 0.562662]\n",
      "[Epoch 0/10] [Batch 33/79] [D loss: 0.455086] [G loss: 0.556032]\n",
      "[Epoch 0/10] [Batch 34/79] [D loss: 0.455350] [G loss: 0.561136]\n",
      "[Epoch 0/10] [Batch 35/79] [D loss: 0.454572] [G loss: 0.565158]\n",
      "[Epoch 0/10] [Batch 36/79] [D loss: 0.455702] [G loss: 0.568388]\n",
      "[Epoch 0/10] [Batch 37/79] [D loss: 0.449756] [G loss: 0.582958]\n",
      "[Epoch 0/10] [Batch 38/79] [D loss: 0.458794] [G loss: 0.573023]\n",
      "[Epoch 0/10] [Batch 39/79] [D loss: 0.458814] [G loss: 0.577425]\n",
      "[Epoch 0/10] [Batch 40/79] [D loss: 0.459533] [G loss: 0.577978]\n",
      "[Epoch 0/10] [Batch 41/79] [D loss: 0.465411] [G loss: 0.578836]\n",
      "[Epoch 0/10] [Batch 42/79] [D loss: 0.460001] [G loss: 0.588374]\n",
      "[Epoch 0/10] [Batch 43/79] [D loss: 0.458532] [G loss: 0.584128]\n",
      "[Epoch 0/10] [Batch 44/79] [D loss: 0.461085] [G loss: 0.585355]\n",
      "[Epoch 0/10] [Batch 45/79] [D loss: 0.455580] [G loss: 0.589694]\n",
      "[Epoch 0/10] [Batch 46/79] [D loss: 0.455463] [G loss: 0.592986]\n",
      "[Epoch 0/10] [Batch 47/79] [D loss: 0.452136] [G loss: 0.594475]\n",
      "[Epoch 0/10] [Batch 48/79] [D loss: 0.448366] [G loss: 0.603858]\n",
      "[Epoch 0/10] [Batch 49/79] [D loss: 0.448304] [G loss: 0.603819]\n",
      "[Epoch 0/10] [Batch 50/79] [D loss: 0.443985] [G loss: 0.607840]\n",
      "[Epoch 0/10] [Batch 51/79] [D loss: 0.444590] [G loss: 0.603357]\n",
      "[Epoch 0/10] [Batch 52/79] [D loss: 0.444564] [G loss: 0.605298]\n",
      "[Epoch 0/10] [Batch 53/79] [D loss: 0.446557] [G loss: 0.604842]\n",
      "[Epoch 0/10] [Batch 54/79] [D loss: 0.445564] [G loss: 0.607827]\n",
      "[Epoch 0/10] [Batch 55/79] [D loss: 0.443322] [G loss: 0.614003]\n",
      "[Epoch 0/10] [Batch 56/79] [D loss: 0.443549] [G loss: 0.612545]\n",
      "[Epoch 0/10] [Batch 57/79] [D loss: 0.446612] [G loss: 0.611995]\n",
      "[Epoch 0/10] [Batch 58/79] [D loss: 0.444331] [G loss: 0.613259]\n",
      "[Epoch 0/10] [Batch 59/79] [D loss: 0.439131] [G loss: 0.619688]\n",
      "[Epoch 0/10] [Batch 60/79] [D loss: 0.442565] [G loss: 0.616938]\n",
      "[Epoch 0/10] [Batch 61/79] [D loss: 0.441039] [G loss: 0.622287]\n",
      "[Epoch 0/10] [Batch 62/79] [D loss: 0.440043] [G loss: 0.621829]\n",
      "[Epoch 0/10] [Batch 63/79] [D loss: 0.443336] [G loss: 0.621664]\n",
      "[Epoch 0/10] [Batch 64/79] [D loss: 0.439794] [G loss: 0.623851]\n",
      "[Epoch 0/10] [Batch 65/79] [D loss: 0.435020] [G loss: 0.629885]\n",
      "[Epoch 0/10] [Batch 66/79] [D loss: 0.433244] [G loss: 0.634171]\n",
      "[Epoch 0/10] [Batch 67/79] [D loss: 0.430201] [G loss: 0.644437]\n",
      "[Epoch 0/10] [Batch 68/79] [D loss: 0.431966] [G loss: 0.657348]\n",
      "[Epoch 0/10] [Batch 69/79] [D loss: 0.425014] [G loss: 0.658994]\n",
      "[Epoch 0/10] [Batch 70/79] [D loss: 0.419042] [G loss: 0.661312]\n",
      "[Epoch 0/10] [Batch 71/79] [D loss: 0.416359] [G loss: 0.667200]\n",
      "[Epoch 0/10] [Batch 72/79] [D loss: 0.411440] [G loss: 0.682210]\n",
      "[Epoch 0/10] [Batch 73/79] [D loss: 0.414206] [G loss: 0.685689]\n",
      "[Epoch 0/10] [Batch 74/79] [D loss: 0.411493] [G loss: 0.686228]\n",
      "[Epoch 0/10] [Batch 75/79] [D loss: 0.405313] [G loss: 0.694284]\n",
      "[Epoch 0/10] [Batch 76/79] [D loss: 0.401901] [G loss: 0.704347]\n",
      "[Epoch 0/10] [Batch 77/79] [D loss: 0.404391] [G loss: 0.711090]\n",
      "[Epoch 0/10] [Batch 78/79] [D loss: 0.398616] [G loss: 0.709907]\n",
      "[Epoch 1/10] [Batch 0/79] [D loss: 0.399583] [G loss: 0.712868]\n",
      "[Epoch 1/10] [Batch 1/79] [D loss: 0.400017] [G loss: 0.714616]\n",
      "[Epoch 1/10] [Batch 2/79] [D loss: 0.397469] [G loss: 0.707766]\n",
      "[Epoch 1/10] [Batch 3/79] [D loss: 0.396257] [G loss: 0.704465]\n",
      "[Epoch 1/10] [Batch 4/79] [D loss: 0.396407] [G loss: 0.708396]\n",
      "[Epoch 1/10] [Batch 5/79] [D loss: 0.392007] [G loss: 0.714296]\n",
      "[Epoch 1/10] [Batch 6/79] [D loss: 0.382387] [G loss: 0.722113]\n",
      "[Epoch 1/10] [Batch 7/79] [D loss: 0.378560] [G loss: 0.734407]\n",
      "[Epoch 1/10] [Batch 8/79] [D loss: 0.374476] [G loss: 0.745094]\n",
      "[Epoch 1/10] [Batch 9/79] [D loss: 0.365511] [G loss: 0.753538]\n",
      "[Epoch 1/10] [Batch 10/79] [D loss: 0.359880] [G loss: 0.763943]\n",
      "[Epoch 1/10] [Batch 11/79] [D loss: 0.350641] [G loss: 0.775836]\n",
      "[Epoch 1/10] [Batch 12/79] [D loss: 0.343102] [G loss: 0.781459]\n",
      "[Epoch 1/10] [Batch 13/79] [D loss: 0.337596] [G loss: 0.804601]\n",
      "[Epoch 1/10] [Batch 14/79] [D loss: 0.326038] [G loss: 0.819746]\n",
      "[Epoch 1/10] [Batch 15/79] [D loss: 0.325163] [G loss: 0.829322]\n",
      "[Epoch 1/10] [Batch 16/79] [D loss: 0.323900] [G loss: 0.830175]\n",
      "[Epoch 1/10] [Batch 17/79] [D loss: 0.328114] [G loss: 0.819427]\n",
      "[Epoch 1/10] [Batch 18/79] [D loss: 0.330519] [G loss: 0.809932]\n",
      "[Epoch 1/10] [Batch 19/79] [D loss: 0.329346] [G loss: 0.803083]\n",
      "[Epoch 1/10] [Batch 20/79] [D loss: 0.328152] [G loss: 0.813137]\n",
      "[Epoch 1/10] [Batch 21/79] [D loss: 0.330061] [G loss: 0.818766]\n",
      "[Epoch 1/10] [Batch 22/79] [D loss: 0.328871] [G loss: 0.818717]\n",
      "[Epoch 1/10] [Batch 23/79] [D loss: 0.325641] [G loss: 0.813394]\n",
      "[Epoch 1/10] [Batch 24/79] [D loss: 0.331946] [G loss: 0.805978]\n",
      "[Epoch 1/10] [Batch 25/79] [D loss: 0.327891] [G loss: 0.808913]\n",
      "[Epoch 1/10] [Batch 26/79] [D loss: 0.330856] [G loss: 0.808977]\n",
      "[Epoch 1/10] [Batch 27/79] [D loss: 0.334305] [G loss: 0.806210]\n",
      "[Epoch 1/10] [Batch 28/79] [D loss: 0.334510] [G loss: 0.796993]\n",
      "[Epoch 1/10] [Batch 29/79] [D loss: 0.342939] [G loss: 0.797948]\n",
      "[Epoch 1/10] [Batch 30/79] [D loss: 0.337285] [G loss: 0.793481]\n",
      "[Epoch 1/10] [Batch 31/79] [D loss: 0.326843] [G loss: 0.810607]\n",
      "[Epoch 1/10] [Batch 32/79] [D loss: 0.323494] [G loss: 0.828965]\n",
      "[Epoch 1/10] [Batch 33/79] [D loss: 0.323854] [G loss: 0.844535]\n",
      "[Epoch 1/10] [Batch 34/79] [D loss: 0.312977] [G loss: 0.853175]\n",
      "[Epoch 1/10] [Batch 35/79] [D loss: 0.306689] [G loss: 0.856938]\n",
      "[Epoch 1/10] [Batch 36/79] [D loss: 0.298891] [G loss: 0.882287]\n",
      "[Epoch 1/10] [Batch 37/79] [D loss: 0.295496] [G loss: 0.901549]\n",
      "[Epoch 1/10] [Batch 38/79] [D loss: 0.291370] [G loss: 0.908848]\n",
      "[Epoch 1/10] [Batch 39/79] [D loss: 0.280670] [G loss: 0.926742]\n",
      "[Epoch 1/10] [Batch 40/79] [D loss: 0.280214] [G loss: 0.932697]\n",
      "[Epoch 1/10] [Batch 41/79] [D loss: 0.274491] [G loss: 0.949603]\n",
      "[Epoch 1/10] [Batch 42/79] [D loss: 0.278115] [G loss: 0.943815]\n",
      "[Epoch 1/10] [Batch 43/79] [D loss: 0.281700] [G loss: 0.942960]\n",
      "[Epoch 1/10] [Batch 44/79] [D loss: 0.284132] [G loss: 0.924119]\n",
      "[Epoch 1/10] [Batch 45/79] [D loss: 0.285987] [G loss: 0.926366]\n",
      "[Epoch 1/10] [Batch 46/79] [D loss: 0.286333] [G loss: 0.930886]\n",
      "[Epoch 1/10] [Batch 47/79] [D loss: 0.285290] [G loss: 0.927016]\n",
      "[Epoch 1/10] [Batch 48/79] [D loss: 0.282406] [G loss: 0.919499]\n",
      "[Epoch 1/10] [Batch 49/79] [D loss: 0.279537] [G loss: 0.940521]\n",
      "[Epoch 1/10] [Batch 50/79] [D loss: 0.282042] [G loss: 0.927300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] [Batch 51/79] [D loss: 0.282806] [G loss: 0.929213]\n",
      "[Epoch 1/10] [Batch 52/79] [D loss: 0.285075] [G loss: 0.924746]\n",
      "[Epoch 1/10] [Batch 53/79] [D loss: 0.280509] [G loss: 0.919785]\n",
      "[Epoch 1/10] [Batch 54/79] [D loss: 0.276718] [G loss: 0.938131]\n",
      "[Epoch 1/10] [Batch 55/79] [D loss: 0.268490] [G loss: 0.968905]\n",
      "[Epoch 1/10] [Batch 56/79] [D loss: 0.265905] [G loss: 0.985333]\n",
      "[Epoch 1/10] [Batch 57/79] [D loss: 0.262630] [G loss: 0.978073]\n",
      "[Epoch 1/10] [Batch 58/79] [D loss: 0.256958] [G loss: 0.995288]\n",
      "[Epoch 1/10] [Batch 59/79] [D loss: 0.252840] [G loss: 1.016137]\n",
      "[Epoch 1/10] [Batch 60/79] [D loss: 0.248391] [G loss: 1.044282]\n",
      "[Epoch 1/10] [Batch 61/79] [D loss: 0.254468] [G loss: 1.027235]\n",
      "[Epoch 1/10] [Batch 62/79] [D loss: 0.254051] [G loss: 1.010519]\n",
      "[Epoch 1/10] [Batch 63/79] [D loss: 0.258900] [G loss: 1.000717]\n",
      "[Epoch 1/10] [Batch 64/79] [D loss: 0.257402] [G loss: 1.017622]\n",
      "[Epoch 1/10] [Batch 65/79] [D loss: 0.256615] [G loss: 1.014609]\n",
      "[Epoch 1/10] [Batch 66/79] [D loss: 0.256391] [G loss: 1.009569]\n",
      "[Epoch 1/10] [Batch 67/79] [D loss: 0.249690] [G loss: 1.027566]\n",
      "[Epoch 1/10] [Batch 68/79] [D loss: 0.245059] [G loss: 1.036479]\n",
      "[Epoch 1/10] [Batch 69/79] [D loss: 0.238494] [G loss: 1.057281]\n",
      "[Epoch 1/10] [Batch 70/79] [D loss: 0.237567] [G loss: 1.079317]\n",
      "[Epoch 1/10] [Batch 71/79] [D loss: 0.233806] [G loss: 1.085464]\n",
      "[Epoch 1/10] [Batch 72/79] [D loss: 0.227078] [G loss: 1.084434]\n",
      "[Epoch 1/10] [Batch 73/79] [D loss: 0.223929] [G loss: 1.097397]\n",
      "[Epoch 1/10] [Batch 74/79] [D loss: 0.221595] [G loss: 1.128933]\n",
      "[Epoch 1/10] [Batch 75/79] [D loss: 0.216027] [G loss: 1.151357]\n",
      "[Epoch 1/10] [Batch 76/79] [D loss: 0.215185] [G loss: 1.165852]\n",
      "[Epoch 1/10] [Batch 77/79] [D loss: 0.209098] [G loss: 1.163506]\n",
      "[Epoch 1/10] [Batch 78/79] [D loss: 0.201928] [G loss: 1.178968]\n",
      "[Epoch 2/10] [Batch 0/79] [D loss: 0.206465] [G loss: 1.234003]\n",
      "[Epoch 2/10] [Batch 1/79] [D loss: 0.203098] [G loss: 1.203735]\n",
      "[Epoch 2/10] [Batch 2/79] [D loss: 0.201264] [G loss: 1.198807]\n",
      "[Epoch 2/10] [Batch 3/79] [D loss: 0.200128] [G loss: 1.219919]\n",
      "[Epoch 2/10] [Batch 4/79] [D loss: 0.201864] [G loss: 1.226655]\n",
      "[Epoch 2/10] [Batch 5/79] [D loss: 0.206581] [G loss: 1.193654]\n",
      "[Epoch 2/10] [Batch 6/79] [D loss: 0.202578] [G loss: 1.188228]\n",
      "[Epoch 2/10] [Batch 7/79] [D loss: 0.204524] [G loss: 1.211690]\n",
      "[Epoch 2/10] [Batch 8/79] [D loss: 0.200084] [G loss: 1.219145]\n",
      "[Epoch 2/10] [Batch 9/79] [D loss: 0.196013] [G loss: 1.245401]\n",
      "[Epoch 2/10] [Batch 10/79] [D loss: 0.189547] [G loss: 1.263013]\n",
      "[Epoch 2/10] [Batch 11/79] [D loss: 0.189288] [G loss: 1.298738]\n",
      "[Epoch 2/10] [Batch 12/79] [D loss: 0.186358] [G loss: 1.281516]\n",
      "[Epoch 2/10] [Batch 13/79] [D loss: 0.184002] [G loss: 1.301668]\n",
      "[Epoch 2/10] [Batch 14/79] [D loss: 0.175020] [G loss: 1.354199]\n",
      "[Epoch 2/10] [Batch 15/79] [D loss: 0.166759] [G loss: 1.380185]\n",
      "[Epoch 2/10] [Batch 16/79] [D loss: 0.163658] [G loss: 1.425229]\n",
      "[Epoch 2/10] [Batch 17/79] [D loss: 0.162172] [G loss: 1.421862]\n",
      "[Epoch 2/10] [Batch 18/79] [D loss: 0.160250] [G loss: 1.393015]\n",
      "[Epoch 2/10] [Batch 19/79] [D loss: 0.156305] [G loss: 1.432720]\n",
      "[Epoch 2/10] [Batch 20/79] [D loss: 0.155442] [G loss: 1.476328]\n",
      "[Epoch 2/10] [Batch 21/79] [D loss: 0.148627] [G loss: 1.456283]\n",
      "[Epoch 2/10] [Batch 22/79] [D loss: 0.148537] [G loss: 1.494902]\n",
      "[Epoch 2/10] [Batch 23/79] [D loss: 0.145532] [G loss: 1.498755]\n",
      "[Epoch 2/10] [Batch 24/79] [D loss: 0.145151] [G loss: 1.502695]\n",
      "[Epoch 2/10] [Batch 25/79] [D loss: 0.146299] [G loss: 1.485908]\n",
      "[Epoch 2/10] [Batch 26/79] [D loss: 0.147130] [G loss: 1.474665]\n",
      "[Epoch 2/10] [Batch 27/79] [D loss: 0.148921] [G loss: 1.481069]\n",
      "[Epoch 2/10] [Batch 28/79] [D loss: 0.149374] [G loss: 1.460282]\n",
      "[Epoch 2/10] [Batch 29/79] [D loss: 0.145274] [G loss: 1.484033]\n",
      "[Epoch 2/10] [Batch 30/79] [D loss: 0.142606] [G loss: 1.500856]\n",
      "[Epoch 2/10] [Batch 31/79] [D loss: 0.141687] [G loss: 1.502099]\n",
      "[Epoch 2/10] [Batch 32/79] [D loss: 0.137645] [G loss: 1.524542]\n",
      "[Epoch 2/10] [Batch 33/79] [D loss: 0.137328] [G loss: 1.545429]\n",
      "[Epoch 2/10] [Batch 34/79] [D loss: 0.134549] [G loss: 1.552448]\n",
      "[Epoch 2/10] [Batch 35/79] [D loss: 0.131757] [G loss: 1.564336]\n",
      "[Epoch 2/10] [Batch 36/79] [D loss: 0.130565] [G loss: 1.603773]\n",
      "[Epoch 2/10] [Batch 37/79] [D loss: 0.128699] [G loss: 1.602644]\n",
      "[Epoch 2/10] [Batch 38/79] [D loss: 0.126196] [G loss: 1.630889]\n",
      "[Epoch 2/10] [Batch 39/79] [D loss: 0.128594] [G loss: 1.617936]\n",
      "[Epoch 2/10] [Batch 40/79] [D loss: 0.124528] [G loss: 1.629983]\n",
      "[Epoch 2/10] [Batch 41/79] [D loss: 0.128630] [G loss: 1.619578]\n",
      "[Epoch 2/10] [Batch 42/79] [D loss: 0.127236] [G loss: 1.593542]\n",
      "[Epoch 2/10] [Batch 43/79] [D loss: 0.123948] [G loss: 1.631961]\n",
      "[Epoch 2/10] [Batch 44/79] [D loss: 0.125962] [G loss: 1.624236]\n",
      "[Epoch 2/10] [Batch 45/79] [D loss: 0.125600] [G loss: 1.604028]\n",
      "[Epoch 2/10] [Batch 46/79] [D loss: 0.122333] [G loss: 1.615106]\n",
      "[Epoch 2/10] [Batch 47/79] [D loss: 0.119330] [G loss: 1.661603]\n",
      "[Epoch 2/10] [Batch 48/79] [D loss: 0.114070] [G loss: 1.706503]\n",
      "[Epoch 2/10] [Batch 49/79] [D loss: 0.108412] [G loss: 1.736707]\n",
      "[Epoch 2/10] [Batch 50/79] [D loss: 0.105852] [G loss: 1.768184]\n",
      "[Epoch 2/10] [Batch 51/79] [D loss: 0.097387] [G loss: 1.841754]\n",
      "[Epoch 2/10] [Batch 52/79] [D loss: 0.095237] [G loss: 1.895366]\n",
      "[Epoch 2/10] [Batch 53/79] [D loss: 0.093111] [G loss: 1.903067]\n",
      "[Epoch 2/10] [Batch 54/79] [D loss: 0.093126] [G loss: 1.918536]\n",
      "[Epoch 2/10] [Batch 55/79] [D loss: 0.093693] [G loss: 1.875645]\n",
      "[Epoch 2/10] [Batch 56/79] [D loss: 0.093196] [G loss: 1.862649]\n",
      "[Epoch 2/10] [Batch 57/79] [D loss: 0.093569] [G loss: 1.884626]\n",
      "[Epoch 2/10] [Batch 58/79] [D loss: 0.095325] [G loss: 1.879004]\n",
      "[Epoch 2/10] [Batch 59/79] [D loss: 0.093729] [G loss: 1.855445]\n",
      "[Epoch 2/10] [Batch 60/79] [D loss: 0.093013] [G loss: 1.850057]\n",
      "[Epoch 2/10] [Batch 61/79] [D loss: 0.091870] [G loss: 1.884344]\n",
      "[Epoch 2/10] [Batch 62/79] [D loss: 0.091334] [G loss: 1.916690]\n",
      "[Epoch 2/10] [Batch 63/79] [D loss: 0.089658] [G loss: 1.897035]\n",
      "[Epoch 2/10] [Batch 64/79] [D loss: 0.088401] [G loss: 1.912465]\n",
      "[Epoch 2/10] [Batch 65/79] [D loss: 0.087668] [G loss: 1.928143]\n",
      "[Epoch 2/10] [Batch 66/79] [D loss: 0.086570] [G loss: 1.926535]\n",
      "[Epoch 2/10] [Batch 67/79] [D loss: 0.086352] [G loss: 1.928542]\n",
      "[Epoch 2/10] [Batch 68/79] [D loss: 0.085809] [G loss: 1.940444]\n",
      "[Epoch 2/10] [Batch 69/79] [D loss: 0.082340] [G loss: 1.965998]\n",
      "[Epoch 2/10] [Batch 70/79] [D loss: 0.080089] [G loss: 1.991929]\n",
      "[Epoch 2/10] [Batch 71/79] [D loss: 0.079718] [G loss: 2.024742]\n",
      "[Epoch 2/10] [Batch 72/79] [D loss: 0.078000] [G loss: 2.027942]\n",
      "[Epoch 2/10] [Batch 73/79] [D loss: 0.075659] [G loss: 2.050303]\n",
      "[Epoch 2/10] [Batch 74/79] [D loss: 0.073146] [G loss: 2.066346]\n",
      "[Epoch 2/10] [Batch 75/79] [D loss: 0.070189] [G loss: 2.116339]\n",
      "[Epoch 2/10] [Batch 76/79] [D loss: 0.068693] [G loss: 2.160018]\n",
      "[Epoch 2/10] [Batch 77/79] [D loss: 0.064791] [G loss: 2.185991]\n",
      "[Epoch 2/10] [Batch 78/79] [D loss: 0.063176] [G loss: 2.224210]\n",
      "[Epoch 3/10] [Batch 0/79] [D loss: 0.060530] [G loss: 2.237072]\n",
      "[Epoch 3/10] [Batch 1/79] [D loss: 0.059661] [G loss: 2.259906]\n",
      "[Epoch 3/10] [Batch 2/79] [D loss: 0.059386] [G loss: 2.282918]\n",
      "[Epoch 3/10] [Batch 3/79] [D loss: 0.059801] [G loss: 2.286458]\n",
      "[Epoch 3/10] [Batch 4/79] [D loss: 0.058397] [G loss: 2.293163]\n",
      "[Epoch 3/10] [Batch 5/79] [D loss: 0.057120] [G loss: 2.300161]\n",
      "[Epoch 3/10] [Batch 6/79] [D loss: 0.056937] [G loss: 2.313242]\n",
      "[Epoch 3/10] [Batch 7/79] [D loss: 0.056945] [G loss: 2.326390]\n",
      "[Epoch 3/10] [Batch 8/79] [D loss: 0.056426] [G loss: 2.310517]\n",
      "[Epoch 3/10] [Batch 9/79] [D loss: 0.057990] [G loss: 2.294118]\n",
      "[Epoch 3/10] [Batch 10/79] [D loss: 0.059530] [G loss: 2.263761]\n",
      "[Epoch 3/10] [Batch 11/79] [D loss: 0.061047] [G loss: 2.239124]\n",
      "[Epoch 3/10] [Batch 12/79] [D loss: 0.061331] [G loss: 2.242064]\n",
      "[Epoch 3/10] [Batch 13/79] [D loss: 0.060800] [G loss: 2.258438]\n",
      "[Epoch 3/10] [Batch 14/79] [D loss: 0.059002] [G loss: 2.265766]\n",
      "[Epoch 3/10] [Batch 15/79] [D loss: 0.058211] [G loss: 2.290460]\n",
      "[Epoch 3/10] [Batch 16/79] [D loss: 0.058223] [G loss: 2.298391]\n",
      "[Epoch 3/10] [Batch 17/79] [D loss: 0.056818] [G loss: 2.301677]\n",
      "[Epoch 3/10] [Batch 18/79] [D loss: 0.057834] [G loss: 2.307003]\n",
      "[Epoch 3/10] [Batch 19/79] [D loss: 0.056009] [G loss: 2.311698]\n",
      "[Epoch 3/10] [Batch 20/79] [D loss: 0.054975] [G loss: 2.334527]\n",
      "[Epoch 3/10] [Batch 21/79] [D loss: 0.055062] [G loss: 2.346967]\n",
      "[Epoch 3/10] [Batch 22/79] [D loss: 0.054090] [G loss: 2.363588]\n",
      "[Epoch 3/10] [Batch 23/79] [D loss: 0.053871] [G loss: 2.347745]\n",
      "[Epoch 3/10] [Batch 24/79] [D loss: 0.053788] [G loss: 2.367313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/10] [Batch 25/79] [D loss: 0.053580] [G loss: 2.369500]\n",
      "[Epoch 3/10] [Batch 26/79] [D loss: 0.053778] [G loss: 2.369090]\n",
      "[Epoch 3/10] [Batch 27/79] [D loss: 0.052457] [G loss: 2.376623]\n",
      "[Epoch 3/10] [Batch 28/79] [D loss: 0.051094] [G loss: 2.402036]\n",
      "[Epoch 3/10] [Batch 29/79] [D loss: 0.050422] [G loss: 2.438753]\n",
      "[Epoch 3/10] [Batch 30/79] [D loss: 0.048711] [G loss: 2.460680]\n",
      "[Epoch 3/10] [Batch 31/79] [D loss: 0.048146] [G loss: 2.480382]\n",
      "[Epoch 3/10] [Batch 32/79] [D loss: 0.046464] [G loss: 2.501016]\n",
      "[Epoch 3/10] [Batch 33/79] [D loss: 0.045299] [G loss: 2.527896]\n",
      "[Epoch 3/10] [Batch 34/79] [D loss: 0.043213] [G loss: 2.567840]\n",
      "[Epoch 3/10] [Batch 35/79] [D loss: 0.042387] [G loss: 2.597712]\n",
      "[Epoch 3/10] [Batch 36/79] [D loss: 0.041884] [G loss: 2.612894]\n",
      "[Epoch 3/10] [Batch 37/79] [D loss: 0.041345] [G loss: 2.617947]\n",
      "[Epoch 3/10] [Batch 38/79] [D loss: 0.041069] [G loss: 2.623634]\n",
      "[Epoch 3/10] [Batch 39/79] [D loss: 0.041015] [G loss: 2.632008]\n",
      "[Epoch 3/10] [Batch 40/79] [D loss: 0.040513] [G loss: 2.634915]\n",
      "[Epoch 3/10] [Batch 41/79] [D loss: 0.040741] [G loss: 2.647029]\n",
      "[Epoch 3/10] [Batch 42/79] [D loss: 0.039957] [G loss: 2.662967]\n",
      "[Epoch 3/10] [Batch 43/79] [D loss: 0.039201] [G loss: 2.667933]\n",
      "[Epoch 3/10] [Batch 44/79] [D loss: 0.038876] [G loss: 2.692268]\n",
      "[Epoch 3/10] [Batch 45/79] [D loss: 0.038885] [G loss: 2.698952]\n",
      "[Epoch 3/10] [Batch 46/79] [D loss: 0.038557] [G loss: 2.685831]\n",
      "[Epoch 3/10] [Batch 47/79] [D loss: 0.037981] [G loss: 2.705881]\n",
      "[Epoch 3/10] [Batch 48/79] [D loss: 0.037163] [G loss: 2.718000]\n",
      "[Epoch 3/10] [Batch 49/79] [D loss: 0.036563] [G loss: 2.742284]\n",
      "[Epoch 3/10] [Batch 50/79] [D loss: 0.036538] [G loss: 2.752545]\n",
      "[Epoch 3/10] [Batch 51/79] [D loss: 0.035897] [G loss: 2.763034]\n",
      "[Epoch 3/10] [Batch 52/79] [D loss: 0.035964] [G loss: 2.764493]\n",
      "[Epoch 3/10] [Batch 53/79] [D loss: 0.035846] [G loss: 2.770723]\n",
      "[Epoch 3/10] [Batch 54/79] [D loss: 0.035556] [G loss: 2.758462]\n",
      "[Epoch 3/10] [Batch 55/79] [D loss: 0.035557] [G loss: 2.771329]\n",
      "[Epoch 3/10] [Batch 56/79] [D loss: 0.035935] [G loss: 2.770703]\n",
      "[Epoch 3/10] [Batch 57/79] [D loss: 0.035373] [G loss: 2.776314]\n",
      "[Epoch 3/10] [Batch 58/79] [D loss: 0.035205] [G loss: 2.801047]\n",
      "[Epoch 3/10] [Batch 59/79] [D loss: 0.034492] [G loss: 2.795614]\n",
      "[Epoch 3/10] [Batch 60/79] [D loss: 0.034115] [G loss: 2.810340]\n",
      "[Epoch 3/10] [Batch 61/79] [D loss: 0.033660] [G loss: 2.833746]\n",
      "[Epoch 3/10] [Batch 62/79] [D loss: 0.033148] [G loss: 2.831474]\n",
      "[Epoch 3/10] [Batch 63/79] [D loss: 0.032512] [G loss: 2.848095]\n",
      "[Epoch 3/10] [Batch 64/79] [D loss: 0.032394] [G loss: 2.857745]\n",
      "[Epoch 3/10] [Batch 65/79] [D loss: 0.032420] [G loss: 2.876076]\n",
      "[Epoch 3/10] [Batch 66/79] [D loss: 0.031632] [G loss: 2.879669]\n",
      "[Epoch 3/10] [Batch 67/79] [D loss: 0.031210] [G loss: 2.895087]\n",
      "[Epoch 3/10] [Batch 68/79] [D loss: 0.030424] [G loss: 2.906444]\n",
      "[Epoch 3/10] [Batch 69/79] [D loss: 0.029359] [G loss: 2.952974]\n",
      "[Epoch 3/10] [Batch 70/79] [D loss: 0.029261] [G loss: 2.967907]\n",
      "[Epoch 3/10] [Batch 71/79] [D loss: 0.029047] [G loss: 2.960289]\n",
      "[Epoch 3/10] [Batch 72/79] [D loss: 0.028541] [G loss: 2.981644]\n",
      "[Epoch 3/10] [Batch 73/79] [D loss: 0.028290] [G loss: 2.988901]\n",
      "[Epoch 3/10] [Batch 74/79] [D loss: 0.028068] [G loss: 2.996238]\n",
      "[Epoch 3/10] [Batch 75/79] [D loss: 0.027099] [G loss: 3.025080]\n",
      "[Epoch 3/10] [Batch 76/79] [D loss: 0.026459] [G loss: 3.044313]\n",
      "[Epoch 3/10] [Batch 77/79] [D loss: 0.025738] [G loss: 3.072749]\n",
      "[Epoch 3/10] [Batch 78/79] [D loss: 0.025366] [G loss: 3.108715]\n",
      "[Epoch 4/10] [Batch 0/79] [D loss: 0.025184] [G loss: 3.105722]\n",
      "[Epoch 4/10] [Batch 1/79] [D loss: 0.025090] [G loss: 3.097382]\n",
      "[Epoch 4/10] [Batch 2/79] [D loss: 0.024917] [G loss: 3.107259]\n",
      "[Epoch 4/10] [Batch 3/79] [D loss: 0.024792] [G loss: 3.133416]\n",
      "[Epoch 4/10] [Batch 4/79] [D loss: 0.024205] [G loss: 3.155656]\n",
      "[Epoch 4/10] [Batch 5/79] [D loss: 0.024128] [G loss: 3.138639]\n",
      "[Epoch 4/10] [Batch 6/79] [D loss: 0.023992] [G loss: 3.158173]\n",
      "[Epoch 4/10] [Batch 7/79] [D loss: 0.023688] [G loss: 3.175138]\n",
      "[Epoch 4/10] [Batch 8/79] [D loss: 0.023496] [G loss: 3.177625]\n",
      "[Epoch 4/10] [Batch 9/79] [D loss: 0.023957] [G loss: 3.153337]\n",
      "[Epoch 4/10] [Batch 10/79] [D loss: 0.023930] [G loss: 3.171106]\n",
      "[Epoch 4/10] [Batch 11/79] [D loss: 0.023835] [G loss: 3.155498]\n",
      "[Epoch 4/10] [Batch 12/79] [D loss: 0.023843] [G loss: 3.148501]\n",
      "[Epoch 4/10] [Batch 13/79] [D loss: 0.023325] [G loss: 3.175362]\n",
      "[Epoch 4/10] [Batch 14/79] [D loss: 0.023056] [G loss: 3.180847]\n",
      "[Epoch 4/10] [Batch 15/79] [D loss: 0.022868] [G loss: 3.197993]\n",
      "[Epoch 4/10] [Batch 16/79] [D loss: 0.022933] [G loss: 3.184975]\n",
      "[Epoch 4/10] [Batch 17/79] [D loss: 0.022835] [G loss: 3.174961]\n",
      "[Epoch 4/10] [Batch 18/79] [D loss: 0.022515] [G loss: 3.210732]\n",
      "[Epoch 4/10] [Batch 19/79] [D loss: 0.022236] [G loss: 3.215002]\n",
      "[Epoch 4/10] [Batch 20/79] [D loss: 0.021432] [G loss: 3.238258]\n",
      "[Epoch 4/10] [Batch 21/79] [D loss: 0.020750] [G loss: 3.274339]\n",
      "[Epoch 4/10] [Batch 22/79] [D loss: 0.020830] [G loss: 3.281345]\n",
      "[Epoch 4/10] [Batch 23/79] [D loss: 0.020658] [G loss: 3.290016]\n",
      "[Epoch 4/10] [Batch 24/79] [D loss: 0.020342] [G loss: 3.299074]\n",
      "[Epoch 4/10] [Batch 25/79] [D loss: 0.020083] [G loss: 3.310996]\n",
      "[Epoch 4/10] [Batch 26/79] [D loss: 0.020196] [G loss: 3.298460]\n",
      "[Epoch 4/10] [Batch 27/79] [D loss: 0.020229] [G loss: 3.306087]\n",
      "[Epoch 4/10] [Batch 28/79] [D loss: 0.019906] [G loss: 3.330051]\n",
      "[Epoch 4/10] [Batch 29/79] [D loss: 0.019910] [G loss: 3.328599]\n",
      "[Epoch 4/10] [Batch 30/79] [D loss: 0.019832] [G loss: 3.335155]\n",
      "[Epoch 4/10] [Batch 31/79] [D loss: 0.019727] [G loss: 3.332728]\n",
      "[Epoch 4/10] [Batch 32/79] [D loss: 0.019865] [G loss: 3.322008]\n",
      "[Epoch 4/10] [Batch 33/79] [D loss: 0.019413] [G loss: 3.351249]\n",
      "[Epoch 4/10] [Batch 34/79] [D loss: 0.019340] [G loss: 3.345845]\n",
      "[Epoch 4/10] [Batch 35/79] [D loss: 0.019398] [G loss: 3.355221]\n",
      "[Epoch 4/10] [Batch 36/79] [D loss: 0.019058] [G loss: 3.358244]\n",
      "[Epoch 4/10] [Batch 37/79] [D loss: 0.019042] [G loss: 3.367520]\n",
      "[Epoch 4/10] [Batch 38/79] [D loss: 0.019038] [G loss: 3.366417]\n",
      "[Epoch 4/10] [Batch 39/79] [D loss: 0.018863] [G loss: 3.376209]\n",
      "[Epoch 4/10] [Batch 40/79] [D loss: 0.018654] [G loss: 3.394256]\n",
      "[Epoch 4/10] [Batch 41/79] [D loss: 0.018288] [G loss: 3.403230]\n",
      "[Epoch 4/10] [Batch 42/79] [D loss: 0.018108] [G loss: 3.441597]\n",
      "[Epoch 4/10] [Batch 43/79] [D loss: 0.017855] [G loss: 3.441147]\n",
      "[Epoch 4/10] [Batch 44/79] [D loss: 0.017382] [G loss: 3.458799]\n",
      "[Epoch 4/10] [Batch 45/79] [D loss: 0.017295] [G loss: 3.462903]\n",
      "[Epoch 4/10] [Batch 46/79] [D loss: 0.016756] [G loss: 3.505243]\n",
      "[Epoch 4/10] [Batch 47/79] [D loss: 0.016577] [G loss: 3.500752]\n",
      "[Epoch 4/10] [Batch 48/79] [D loss: 0.016674] [G loss: 3.516063]\n",
      "[Epoch 4/10] [Batch 49/79] [D loss: 0.016531] [G loss: 3.528108]\n",
      "[Epoch 4/10] [Batch 50/79] [D loss: 0.016092] [G loss: 3.535903]\n",
      "[Epoch 4/10] [Batch 51/79] [D loss: 0.015860] [G loss: 3.549405]\n",
      "[Epoch 4/10] [Batch 52/79] [D loss: 0.015614] [G loss: 3.571080]\n",
      "[Epoch 4/10] [Batch 53/79] [D loss: 0.015513] [G loss: 3.579811]\n",
      "[Epoch 4/10] [Batch 54/79] [D loss: 0.015498] [G loss: 3.578198]\n",
      "[Epoch 4/10] [Batch 55/79] [D loss: 0.015302] [G loss: 3.589018]\n",
      "[Epoch 4/10] [Batch 56/79] [D loss: 0.015483] [G loss: 3.591644]\n",
      "[Epoch 4/10] [Batch 57/79] [D loss: 0.014943] [G loss: 3.622637]\n",
      "[Epoch 4/10] [Batch 58/79] [D loss: 0.015916] [G loss: 3.567233]\n",
      "[Epoch 4/10] [Batch 59/79] [D loss: 0.015414] [G loss: 3.579136]\n",
      "[Epoch 4/10] [Batch 60/79] [D loss: 0.015162] [G loss: 3.594375]\n",
      "[Epoch 4/10] [Batch 61/79] [D loss: 0.015255] [G loss: 3.589865]\n",
      "[Epoch 4/10] [Batch 62/79] [D loss: 0.015181] [G loss: 3.611744]\n",
      "[Epoch 4/10] [Batch 63/79] [D loss: 0.015222] [G loss: 3.596023]\n",
      "[Epoch 4/10] [Batch 64/79] [D loss: 0.015090] [G loss: 3.601465]\n",
      "[Epoch 4/10] [Batch 65/79] [D loss: 0.014897] [G loss: 3.616036]\n",
      "[Epoch 4/10] [Batch 66/79] [D loss: 0.014896] [G loss: 3.627487]\n",
      "[Epoch 4/10] [Batch 67/79] [D loss: 0.014743] [G loss: 3.620001]\n",
      "[Epoch 4/10] [Batch 68/79] [D loss: 0.014931] [G loss: 3.635235]\n",
      "[Epoch 4/10] [Batch 69/79] [D loss: 0.014584] [G loss: 3.653919]\n",
      "[Epoch 4/10] [Batch 70/79] [D loss: 0.014501] [G loss: 3.641272]\n",
      "[Epoch 4/10] [Batch 71/79] [D loss: 0.013973] [G loss: 3.683474]\n",
      "[Epoch 4/10] [Batch 72/79] [D loss: 0.013981] [G loss: 3.683709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/10] [Batch 73/79] [D loss: 0.013673] [G loss: 3.722352]\n",
      "[Epoch 4/10] [Batch 74/79] [D loss: 0.013455] [G loss: 3.727642]\n",
      "[Epoch 4/10] [Batch 75/79] [D loss: 0.013121] [G loss: 3.744153]\n",
      "[Epoch 4/10] [Batch 76/79] [D loss: 0.013089] [G loss: 3.763997]\n",
      "[Epoch 4/10] [Batch 77/79] [D loss: 0.012567] [G loss: 3.786461]\n",
      "[Epoch 4/10] [Batch 78/79] [D loss: 0.012084] [G loss: 3.816429]\n",
      "[Epoch 5/10] [Batch 0/79] [D loss: 0.012188] [G loss: 3.838733]\n",
      "[Epoch 5/10] [Batch 1/79] [D loss: 0.012145] [G loss: 3.841089]\n",
      "[Epoch 5/10] [Batch 2/79] [D loss: 0.012124] [G loss: 3.840488]\n",
      "[Epoch 5/10] [Batch 3/79] [D loss: 0.012047] [G loss: 3.838442]\n",
      "[Epoch 5/10] [Batch 4/79] [D loss: 0.011912] [G loss: 3.862681]\n",
      "[Epoch 5/10] [Batch 5/79] [D loss: 0.011753] [G loss: 3.870749]\n",
      "[Epoch 5/10] [Batch 6/79] [D loss: 0.011604] [G loss: 3.881207]\n",
      "[Epoch 5/10] [Batch 7/79] [D loss: 0.011643] [G loss: 3.887780]\n",
      "[Epoch 5/10] [Batch 8/79] [D loss: 0.011656] [G loss: 3.896701]\n",
      "[Epoch 5/10] [Batch 9/79] [D loss: 0.011773] [G loss: 3.884417]\n",
      "[Epoch 5/10] [Batch 10/79] [D loss: 0.011393] [G loss: 3.914059]\n",
      "[Epoch 5/10] [Batch 11/79] [D loss: 0.011567] [G loss: 3.910703]\n",
      "[Epoch 5/10] [Batch 12/79] [D loss: 0.011567] [G loss: 3.891578]\n",
      "[Epoch 5/10] [Batch 13/79] [D loss: 0.011457] [G loss: 3.905677]\n",
      "[Epoch 5/10] [Batch 14/79] [D loss: 0.011227] [G loss: 3.933096]\n",
      "[Epoch 5/10] [Batch 15/79] [D loss: 0.011123] [G loss: 3.935556]\n",
      "[Epoch 5/10] [Batch 16/79] [D loss: 0.010978] [G loss: 3.942715]\n",
      "[Epoch 5/10] [Batch 17/79] [D loss: 0.010894] [G loss: 3.963240]\n",
      "[Epoch 5/10] [Batch 18/79] [D loss: 0.010525] [G loss: 3.991194]\n",
      "[Epoch 5/10] [Batch 19/79] [D loss: 0.010480] [G loss: 3.999456]\n",
      "[Epoch 5/10] [Batch 20/79] [D loss: 0.010246] [G loss: 4.029572]\n",
      "[Epoch 5/10] [Batch 21/79] [D loss: 0.010245] [G loss: 4.016859]\n",
      "[Epoch 5/10] [Batch 22/79] [D loss: 0.009954] [G loss: 4.056783]\n",
      "[Epoch 5/10] [Batch 23/79] [D loss: 0.009913] [G loss: 4.039869]\n",
      "[Epoch 5/10] [Batch 24/79] [D loss: 0.009831] [G loss: 4.079728]\n",
      "[Epoch 5/10] [Batch 25/79] [D loss: 0.009940] [G loss: 4.051185]\n",
      "[Epoch 5/10] [Batch 26/79] [D loss: 0.009881] [G loss: 4.067012]\n",
      "[Epoch 5/10] [Batch 27/79] [D loss: 0.009423] [G loss: 4.097673]\n",
      "[Epoch 5/10] [Batch 28/79] [D loss: 0.009575] [G loss: 4.112500]\n",
      "[Epoch 5/10] [Batch 29/79] [D loss: 0.009540] [G loss: 4.107644]\n",
      "[Epoch 5/10] [Batch 30/79] [D loss: 0.009355] [G loss: 4.110962]\n",
      "[Epoch 5/10] [Batch 31/79] [D loss: 0.009559] [G loss: 4.117102]\n",
      "[Epoch 5/10] [Batch 32/79] [D loss: 0.009468] [G loss: 4.105867]\n",
      "[Epoch 5/10] [Batch 33/79] [D loss: 0.009616] [G loss: 4.096434]\n",
      "[Epoch 5/10] [Batch 34/79] [D loss: 0.009548] [G loss: 4.105735]\n",
      "[Epoch 5/10] [Batch 35/79] [D loss: 0.009336] [G loss: 4.118927]\n",
      "[Epoch 5/10] [Batch 36/79] [D loss: 0.009335] [G loss: 4.117147]\n",
      "[Epoch 5/10] [Batch 37/79] [D loss: 0.009536] [G loss: 4.110853]\n",
      "[Epoch 5/10] [Batch 38/79] [D loss: 0.009370] [G loss: 4.116863]\n",
      "[Epoch 5/10] [Batch 39/79] [D loss: 0.009609] [G loss: 4.099500]\n",
      "[Epoch 5/10] [Batch 40/79] [D loss: 0.009508] [G loss: 4.108356]\n",
      "[Epoch 5/10] [Batch 41/79] [D loss: 0.009423] [G loss: 4.108514]\n",
      "[Epoch 5/10] [Batch 42/79] [D loss: 0.009216] [G loss: 4.126329]\n",
      "[Epoch 5/10] [Batch 43/79] [D loss: 0.009192] [G loss: 4.126735]\n",
      "[Epoch 5/10] [Batch 44/79] [D loss: 0.009105] [G loss: 4.123904]\n",
      "[Epoch 5/10] [Batch 45/79] [D loss: 0.009048] [G loss: 4.148071]\n",
      "[Epoch 5/10] [Batch 46/79] [D loss: 0.008930] [G loss: 4.157207]\n",
      "[Epoch 5/10] [Batch 47/79] [D loss: 0.008822] [G loss: 4.154841]\n",
      "[Epoch 5/10] [Batch 48/79] [D loss: 0.008746] [G loss: 4.167180]\n",
      "[Epoch 5/10] [Batch 49/79] [D loss: 0.008563] [G loss: 4.183008]\n",
      "[Epoch 5/10] [Batch 50/79] [D loss: 0.008564] [G loss: 4.200936]\n",
      "[Epoch 5/10] [Batch 51/79] [D loss: 0.008568] [G loss: 4.198594]\n",
      "[Epoch 5/10] [Batch 52/79] [D loss: 0.008519] [G loss: 4.183550]\n",
      "[Epoch 5/10] [Batch 53/79] [D loss: 0.008472] [G loss: 4.210939]\n",
      "[Epoch 5/10] [Batch 54/79] [D loss: 0.008409] [G loss: 4.210906]\n",
      "[Epoch 5/10] [Batch 55/79] [D loss: 0.008453] [G loss: 4.202448]\n",
      "[Epoch 5/10] [Batch 56/79] [D loss: 0.008388] [G loss: 4.216939]\n",
      "[Epoch 5/10] [Batch 57/79] [D loss: 0.008351] [G loss: 4.208953]\n",
      "[Epoch 5/10] [Batch 58/79] [D loss: 0.008322] [G loss: 4.214758]\n",
      "[Epoch 5/10] [Batch 59/79] [D loss: 0.008439] [G loss: 4.196504]\n",
      "[Epoch 5/10] [Batch 60/79] [D loss: 0.008452] [G loss: 4.207590]\n",
      "[Epoch 5/10] [Batch 61/79] [D loss: 0.008363] [G loss: 4.197089]\n",
      "[Epoch 5/10] [Batch 62/79] [D loss: 0.008254] [G loss: 4.201242]\n",
      "[Epoch 5/10] [Batch 63/79] [D loss: 0.008093] [G loss: 4.232382]\n",
      "[Epoch 5/10] [Batch 64/79] [D loss: 0.008168] [G loss: 4.236656]\n",
      "[Epoch 5/10] [Batch 65/79] [D loss: 0.008189] [G loss: 4.238340]\n",
      "[Epoch 5/10] [Batch 66/79] [D loss: 0.008002] [G loss: 4.249433]\n",
      "[Epoch 5/10] [Batch 67/79] [D loss: 0.008041] [G loss: 4.234351]\n",
      "[Epoch 5/10] [Batch 68/79] [D loss: 0.008104] [G loss: 4.224559]\n",
      "[Epoch 5/10] [Batch 69/79] [D loss: 0.008034] [G loss: 4.251057]\n",
      "[Epoch 5/10] [Batch 70/79] [D loss: 0.007832] [G loss: 4.266136]\n",
      "[Epoch 5/10] [Batch 71/79] [D loss: 0.007816] [G loss: 4.282012]\n",
      "[Epoch 5/10] [Batch 72/79] [D loss: 0.007860] [G loss: 4.285996]\n",
      "[Epoch 5/10] [Batch 73/79] [D loss: 0.007810] [G loss: 4.275226]\n",
      "[Epoch 5/10] [Batch 74/79] [D loss: 0.007684] [G loss: 4.294289]\n",
      "[Epoch 5/10] [Batch 75/79] [D loss: 0.007604] [G loss: 4.297266]\n",
      "[Epoch 5/10] [Batch 76/79] [D loss: 0.007465] [G loss: 4.306450]\n",
      "[Epoch 5/10] [Batch 77/79] [D loss: 0.007372] [G loss: 4.336195]\n",
      "[Epoch 5/10] [Batch 78/79] [D loss: 0.007175] [G loss: 4.350871]\n",
      "[Epoch 6/10] [Batch 0/79] [D loss: 0.007095] [G loss: 4.379891]\n",
      "[Epoch 6/10] [Batch 1/79] [D loss: 0.007294] [G loss: 4.372257]\n",
      "[Epoch 6/10] [Batch 2/79] [D loss: 0.007175] [G loss: 4.369826]\n",
      "[Epoch 6/10] [Batch 3/79] [D loss: 0.006838] [G loss: 4.396566]\n",
      "[Epoch 6/10] [Batch 4/79] [D loss: 0.007002] [G loss: 4.392242]\n",
      "[Epoch 6/10] [Batch 5/79] [D loss: 0.006841] [G loss: 4.420135]\n",
      "[Epoch 6/10] [Batch 6/79] [D loss: 0.006856] [G loss: 4.413823]\n",
      "[Epoch 6/10] [Batch 7/79] [D loss: 0.006781] [G loss: 4.414487]\n",
      "[Epoch 6/10] [Batch 8/79] [D loss: 0.006666] [G loss: 4.436875]\n",
      "[Epoch 6/10] [Batch 9/79] [D loss: 0.006723] [G loss: 4.437434]\n",
      "[Epoch 6/10] [Batch 10/79] [D loss: 0.006611] [G loss: 4.454754]\n",
      "[Epoch 6/10] [Batch 11/79] [D loss: 0.006647] [G loss: 4.436812]\n",
      "[Epoch 6/10] [Batch 12/79] [D loss: 0.006634] [G loss: 4.434078]\n",
      "[Epoch 6/10] [Batch 13/79] [D loss: 0.006361] [G loss: 4.483147]\n",
      "[Epoch 6/10] [Batch 14/79] [D loss: 0.006672] [G loss: 4.437880]\n",
      "[Epoch 6/10] [Batch 15/79] [D loss: 0.006523] [G loss: 4.474208]\n",
      "[Epoch 6/10] [Batch 16/79] [D loss: 0.006540] [G loss: 4.459862]\n",
      "[Epoch 6/10] [Batch 17/79] [D loss: 0.006696] [G loss: 4.437733]\n",
      "[Epoch 6/10] [Batch 18/79] [D loss: 0.006638] [G loss: 4.424765]\n",
      "[Epoch 6/10] [Batch 19/79] [D loss: 0.006757] [G loss: 4.418036]\n",
      "[Epoch 6/10] [Batch 20/79] [D loss: 0.006697] [G loss: 4.433352]\n",
      "[Epoch 6/10] [Batch 21/79] [D loss: 0.006736] [G loss: 4.422858]\n",
      "[Epoch 6/10] [Batch 22/79] [D loss: 0.006636] [G loss: 4.449581]\n",
      "[Epoch 6/10] [Batch 23/79] [D loss: 0.006409] [G loss: 4.463229]\n",
      "[Epoch 6/10] [Batch 24/79] [D loss: 0.006421] [G loss: 4.449332]\n",
      "[Epoch 6/10] [Batch 25/79] [D loss: 0.006446] [G loss: 4.456736]\n",
      "[Epoch 6/10] [Batch 26/79] [D loss: 0.006306] [G loss: 4.469881]\n",
      "[Epoch 6/10] [Batch 27/79] [D loss: 0.006197] [G loss: 4.473881]\n",
      "[Epoch 6/10] [Batch 28/79] [D loss: 0.006035] [G loss: 4.514302]\n",
      "[Epoch 6/10] [Batch 29/79] [D loss: 0.006109] [G loss: 4.504498]\n",
      "[Epoch 6/10] [Batch 30/79] [D loss: 0.006059] [G loss: 4.524945]\n",
      "[Epoch 6/10] [Batch 31/79] [D loss: 0.006171] [G loss: 4.505106]\n",
      "[Epoch 6/10] [Batch 32/79] [D loss: 0.006165] [G loss: 4.487496]\n",
      "[Epoch 6/10] [Batch 33/79] [D loss: 0.006179] [G loss: 4.502782]\n",
      "[Epoch 6/10] [Batch 34/79] [D loss: 0.006230] [G loss: 4.495750]\n",
      "[Epoch 6/10] [Batch 35/79] [D loss: 0.006241] [G loss: 4.497029]\n",
      "[Epoch 6/10] [Batch 36/79] [D loss: 0.006071] [G loss: 4.537223]\n",
      "[Epoch 6/10] [Batch 37/79] [D loss: 0.006019] [G loss: 4.526278]\n",
      "[Epoch 6/10] [Batch 38/79] [D loss: 0.005993] [G loss: 4.532644]\n",
      "[Epoch 6/10] [Batch 39/79] [D loss: 0.005872] [G loss: 4.539804]\n",
      "[Epoch 6/10] [Batch 40/79] [D loss: 0.005943] [G loss: 4.541854]\n",
      "[Epoch 6/10] [Batch 41/79] [D loss: 0.005823] [G loss: 4.566225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/10] [Batch 42/79] [D loss: 0.005677] [G loss: 4.590944]\n",
      "[Epoch 6/10] [Batch 43/79] [D loss: 0.005697] [G loss: 4.592676]\n",
      "[Epoch 6/10] [Batch 44/79] [D loss: 0.005531] [G loss: 4.604144]\n",
      "[Epoch 6/10] [Batch 45/79] [D loss: 0.005612] [G loss: 4.593297]\n",
      "[Epoch 6/10] [Batch 46/79] [D loss: 0.005541] [G loss: 4.623785]\n",
      "[Epoch 6/10] [Batch 47/79] [D loss: 0.005423] [G loss: 4.634469]\n",
      "[Epoch 6/10] [Batch 48/79] [D loss: 0.005538] [G loss: 4.612797]\n",
      "[Epoch 6/10] [Batch 49/79] [D loss: 0.005438] [G loss: 4.622783]\n",
      "[Epoch 6/10] [Batch 50/79] [D loss: 0.005299] [G loss: 4.646350]\n",
      "[Epoch 6/10] [Batch 51/79] [D loss: 0.005444] [G loss: 4.635669]\n",
      "[Epoch 6/10] [Batch 52/79] [D loss: 0.005472] [G loss: 4.631164]\n",
      "[Epoch 6/10] [Batch 53/79] [D loss: 0.005425] [G loss: 4.637330]\n",
      "[Epoch 6/10] [Batch 54/79] [D loss: 0.005540] [G loss: 4.605769]\n",
      "[Epoch 6/10] [Batch 55/79] [D loss: 0.005553] [G loss: 4.615571]\n",
      "[Epoch 6/10] [Batch 56/79] [D loss: 0.005430] [G loss: 4.621800]\n",
      "[Epoch 6/10] [Batch 57/79] [D loss: 0.005397] [G loss: 4.622151]\n",
      "[Epoch 6/10] [Batch 58/79] [D loss: 0.005418] [G loss: 4.622903]\n",
      "[Epoch 6/10] [Batch 59/79] [D loss: 0.005465] [G loss: 4.624726]\n",
      "[Epoch 6/10] [Batch 60/79] [D loss: 0.005313] [G loss: 4.651837]\n",
      "[Epoch 6/10] [Batch 61/79] [D loss: 0.005411] [G loss: 4.646166]\n",
      "[Epoch 6/10] [Batch 62/79] [D loss: 0.005387] [G loss: 4.645503]\n",
      "[Epoch 6/10] [Batch 63/79] [D loss: 0.005413] [G loss: 4.618360]\n",
      "[Epoch 6/10] [Batch 64/79] [D loss: 0.005451] [G loss: 4.611311]\n",
      "[Epoch 6/10] [Batch 65/79] [D loss: 0.005431] [G loss: 4.616458]\n",
      "[Epoch 6/10] [Batch 66/79] [D loss: 0.005366] [G loss: 4.630814]\n",
      "[Epoch 6/10] [Batch 67/79] [D loss: 0.005310] [G loss: 4.648461]\n",
      "[Epoch 6/10] [Batch 68/79] [D loss: 0.005262] [G loss: 4.656412]\n",
      "[Epoch 6/10] [Batch 69/79] [D loss: 0.005173] [G loss: 4.673544]\n",
      "[Epoch 6/10] [Batch 70/79] [D loss: 0.005130] [G loss: 4.670537]\n",
      "[Epoch 6/10] [Batch 71/79] [D loss: 0.005054] [G loss: 4.685624]\n",
      "[Epoch 6/10] [Batch 72/79] [D loss: 0.004939] [G loss: 4.706793]\n",
      "[Epoch 6/10] [Batch 73/79] [D loss: 0.005065] [G loss: 4.695867]\n",
      "[Epoch 6/10] [Batch 74/79] [D loss: 0.005092] [G loss: 4.693523]\n",
      "[Epoch 6/10] [Batch 75/79] [D loss: 0.004983] [G loss: 4.699636]\n",
      "[Epoch 6/10] [Batch 76/79] [D loss: 0.004967] [G loss: 4.701010]\n",
      "[Epoch 6/10] [Batch 77/79] [D loss: 0.004912] [G loss: 4.704956]\n",
      "[Epoch 6/10] [Batch 78/79] [D loss: 0.005016] [G loss: 4.711809]\n",
      "[Epoch 7/10] [Batch 0/79] [D loss: 0.004790] [G loss: 4.720085]\n",
      "[Epoch 7/10] [Batch 1/79] [D loss: 0.004806] [G loss: 4.708776]\n",
      "[Epoch 7/10] [Batch 2/79] [D loss: 0.004757] [G loss: 4.733068]\n",
      "[Epoch 7/10] [Batch 3/79] [D loss: 0.004684] [G loss: 4.756647]\n",
      "[Epoch 7/10] [Batch 4/79] [D loss: 0.004728] [G loss: 4.752872]\n",
      "[Epoch 7/10] [Batch 5/79] [D loss: 0.004604] [G loss: 4.783020]\n",
      "[Epoch 7/10] [Batch 6/79] [D loss: 0.004568] [G loss: 4.790045]\n",
      "[Epoch 7/10] [Batch 7/79] [D loss: 0.004555] [G loss: 4.785890]\n",
      "[Epoch 7/10] [Batch 8/79] [D loss: 0.004654] [G loss: 4.765706]\n",
      "[Epoch 7/10] [Batch 9/79] [D loss: 0.004566] [G loss: 4.775968]\n",
      "[Epoch 7/10] [Batch 10/79] [D loss: 0.004481] [G loss: 4.787406]\n",
      "[Epoch 7/10] [Batch 11/79] [D loss: 0.004449] [G loss: 4.797746]\n",
      "[Epoch 7/10] [Batch 12/79] [D loss: 0.004446] [G loss: 4.797451]\n",
      "[Epoch 7/10] [Batch 13/79] [D loss: 0.004544] [G loss: 4.800598]\n",
      "[Epoch 7/10] [Batch 14/79] [D loss: 0.004388] [G loss: 4.815076]\n",
      "[Epoch 7/10] [Batch 15/79] [D loss: 0.004407] [G loss: 4.826461]\n",
      "[Epoch 7/10] [Batch 16/79] [D loss: 0.004369] [G loss: 4.814140]\n",
      "[Epoch 7/10] [Batch 17/79] [D loss: 0.004315] [G loss: 4.832806]\n",
      "[Epoch 7/10] [Batch 18/79] [D loss: 0.004368] [G loss: 4.823506]\n",
      "[Epoch 7/10] [Batch 19/79] [D loss: 0.004278] [G loss: 4.841708]\n",
      "[Epoch 7/10] [Batch 20/79] [D loss: 0.004273] [G loss: 4.843040]\n",
      "[Epoch 7/10] [Batch 21/79] [D loss: 0.004218] [G loss: 4.856709]\n",
      "[Epoch 7/10] [Batch 22/79] [D loss: 0.004284] [G loss: 4.841700]\n",
      "[Epoch 7/10] [Batch 23/79] [D loss: 0.004283] [G loss: 4.848096]\n",
      "[Epoch 7/10] [Batch 24/79] [D loss: 0.004250] [G loss: 4.859441]\n",
      "[Epoch 7/10] [Batch 25/79] [D loss: 0.004195] [G loss: 4.861532]\n",
      "[Epoch 7/10] [Batch 26/79] [D loss: 0.004226] [G loss: 4.856122]\n",
      "[Epoch 7/10] [Batch 27/79] [D loss: 0.004207] [G loss: 4.861938]\n",
      "[Epoch 7/10] [Batch 28/79] [D loss: 0.004195] [G loss: 4.857591]\n",
      "[Epoch 7/10] [Batch 29/79] [D loss: 0.004187] [G loss: 4.870645]\n",
      "[Epoch 7/10] [Batch 30/79] [D loss: 0.004102] [G loss: 4.890945]\n",
      "[Epoch 7/10] [Batch 31/79] [D loss: 0.004091] [G loss: 4.889387]\n",
      "[Epoch 7/10] [Batch 32/79] [D loss: 0.004104] [G loss: 4.892720]\n",
      "[Epoch 7/10] [Batch 33/79] [D loss: 0.003989] [G loss: 4.912467]\n",
      "[Epoch 7/10] [Batch 34/79] [D loss: 0.004051] [G loss: 4.902331]\n",
      "[Epoch 7/10] [Batch 35/79] [D loss: 0.004043] [G loss: 4.906126]\n",
      "[Epoch 7/10] [Batch 36/79] [D loss: 0.003928] [G loss: 4.924518]\n",
      "[Epoch 7/10] [Batch 37/79] [D loss: 0.004077] [G loss: 4.892316]\n",
      "[Epoch 7/10] [Batch 38/79] [D loss: 0.003957] [G loss: 4.922737]\n",
      "[Epoch 7/10] [Batch 39/79] [D loss: 0.003951] [G loss: 4.923614]\n",
      "[Epoch 7/10] [Batch 40/79] [D loss: 0.003937] [G loss: 4.930354]\n",
      "[Epoch 7/10] [Batch 41/79] [D loss: 0.004020] [G loss: 4.912266]\n",
      "[Epoch 7/10] [Batch 42/79] [D loss: 0.003905] [G loss: 4.938817]\n",
      "[Epoch 7/10] [Batch 43/79] [D loss: 0.003980] [G loss: 4.921587]\n",
      "[Epoch 7/10] [Batch 44/79] [D loss: 0.003858] [G loss: 4.944272]\n",
      "[Epoch 7/10] [Batch 45/79] [D loss: 0.003807] [G loss: 4.961795]\n",
      "[Epoch 7/10] [Batch 46/79] [D loss: 0.003867] [G loss: 4.963734]\n",
      "[Epoch 7/10] [Batch 47/79] [D loss: 0.003798] [G loss: 4.958216]\n",
      "[Epoch 7/10] [Batch 48/79] [D loss: 0.003807] [G loss: 4.959414]\n",
      "[Epoch 7/10] [Batch 49/79] [D loss: 0.003776] [G loss: 4.961148]\n",
      "[Epoch 7/10] [Batch 50/79] [D loss: 0.003789] [G loss: 4.976529]\n",
      "[Epoch 7/10] [Batch 51/79] [D loss: 0.003762] [G loss: 4.975368]\n",
      "[Epoch 7/10] [Batch 52/79] [D loss: 0.003832] [G loss: 4.954223]\n",
      "[Epoch 7/10] [Batch 53/79] [D loss: 0.003779] [G loss: 4.963602]\n",
      "[Epoch 7/10] [Batch 54/79] [D loss: 0.003771] [G loss: 4.980875]\n",
      "[Epoch 7/10] [Batch 55/79] [D loss: 0.003718] [G loss: 4.981832]\n",
      "[Epoch 7/10] [Batch 56/79] [D loss: 0.003725] [G loss: 4.993619]\n",
      "[Epoch 7/10] [Batch 57/79] [D loss: 0.003765] [G loss: 4.986316]\n",
      "[Epoch 7/10] [Batch 58/79] [D loss: 0.003718] [G loss: 4.998443]\n",
      "[Epoch 7/10] [Batch 59/79] [D loss: 0.003621] [G loss: 5.001370]\n",
      "[Epoch 7/10] [Batch 60/79] [D loss: 0.003665] [G loss: 5.007246]\n",
      "[Epoch 7/10] [Batch 61/79] [D loss: 0.003599] [G loss: 5.013991]\n",
      "[Epoch 7/10] [Batch 62/79] [D loss: 0.003544] [G loss: 5.030239]\n",
      "[Epoch 7/10] [Batch 63/79] [D loss: 0.003588] [G loss: 5.032957]\n",
      "[Epoch 7/10] [Batch 64/79] [D loss: 0.003521] [G loss: 5.042156]\n",
      "[Epoch 7/10] [Batch 65/79] [D loss: 0.003454] [G loss: 5.060530]\n",
      "[Epoch 7/10] [Batch 66/79] [D loss: 0.003469] [G loss: 5.055937]\n",
      "[Epoch 7/10] [Batch 67/79] [D loss: 0.003496] [G loss: 5.060703]\n",
      "[Epoch 7/10] [Batch 68/79] [D loss: 0.003446] [G loss: 5.063334]\n",
      "[Epoch 7/10] [Batch 69/79] [D loss: 0.003434] [G loss: 5.070931]\n",
      "[Epoch 7/10] [Batch 70/79] [D loss: 0.003392] [G loss: 5.079294]\n",
      "[Epoch 7/10] [Batch 71/79] [D loss: 0.003469] [G loss: 5.079964]\n",
      "[Epoch 7/10] [Batch 72/79] [D loss: 0.003407] [G loss: 5.083690]\n",
      "[Epoch 7/10] [Batch 73/79] [D loss: 0.003361] [G loss: 5.096343]\n",
      "[Epoch 7/10] [Batch 74/79] [D loss: 0.003393] [G loss: 5.075714]\n",
      "[Epoch 7/10] [Batch 75/79] [D loss: 0.003317] [G loss: 5.096316]\n",
      "[Epoch 7/10] [Batch 76/79] [D loss: 0.003296] [G loss: 5.110490]\n",
      "[Epoch 7/10] [Batch 77/79] [D loss: 0.003283] [G loss: 5.125336]\n",
      "[Epoch 7/10] [Batch 78/79] [D loss: 0.003219] [G loss: 5.146427]\n",
      "[Epoch 8/10] [Batch 0/79] [D loss: 0.003219] [G loss: 5.138331]\n",
      "[Epoch 8/10] [Batch 1/79] [D loss: 0.003190] [G loss: 5.143602]\n",
      "[Epoch 8/10] [Batch 2/79] [D loss: 0.003223] [G loss: 5.125459]\n",
      "[Epoch 8/10] [Batch 3/79] [D loss: 0.003209] [G loss: 5.136562]\n",
      "[Epoch 8/10] [Batch 4/79] [D loss: 0.003144] [G loss: 5.139385]\n",
      "[Epoch 8/10] [Batch 5/79] [D loss: 0.003141] [G loss: 5.154598]\n",
      "[Epoch 8/10] [Batch 6/79] [D loss: 0.003102] [G loss: 5.173923]\n",
      "[Epoch 8/10] [Batch 7/79] [D loss: 0.003116] [G loss: 5.170893]\n",
      "[Epoch 8/10] [Batch 8/79] [D loss: 0.003115] [G loss: 5.171385]\n",
      "[Epoch 8/10] [Batch 9/79] [D loss: 0.003074] [G loss: 5.178730]\n",
      "[Epoch 8/10] [Batch 10/79] [D loss: 0.003064] [G loss: 5.181717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/10] [Batch 11/79] [D loss: 0.003098] [G loss: 5.175171]\n",
      "[Epoch 8/10] [Batch 12/79] [D loss: 0.003076] [G loss: 5.174009]\n",
      "[Epoch 8/10] [Batch 13/79] [D loss: 0.003015] [G loss: 5.187126]\n",
      "[Epoch 8/10] [Batch 14/79] [D loss: 0.003009] [G loss: 5.198951]\n",
      "[Epoch 8/10] [Batch 15/79] [D loss: 0.003025] [G loss: 5.196106]\n",
      "[Epoch 8/10] [Batch 16/79] [D loss: 0.003019] [G loss: 5.199329]\n",
      "[Epoch 8/10] [Batch 17/79] [D loss: 0.002992] [G loss: 5.221085]\n",
      "[Epoch 8/10] [Batch 18/79] [D loss: 0.002970] [G loss: 5.212900]\n",
      "[Epoch 8/10] [Batch 19/79] [D loss: 0.002992] [G loss: 5.205290]\n",
      "[Epoch 8/10] [Batch 20/79] [D loss: 0.002985] [G loss: 5.212064]\n",
      "[Epoch 8/10] [Batch 21/79] [D loss: 0.002946] [G loss: 5.220957]\n",
      "[Epoch 8/10] [Batch 22/79] [D loss: 0.002953] [G loss: 5.214458]\n",
      "[Epoch 8/10] [Batch 23/79] [D loss: 0.002943] [G loss: 5.227901]\n",
      "[Epoch 8/10] [Batch 24/79] [D loss: 0.002944] [G loss: 5.227380]\n",
      "[Epoch 8/10] [Batch 25/79] [D loss: 0.002884] [G loss: 5.235212]\n",
      "[Epoch 8/10] [Batch 26/79] [D loss: 0.002951] [G loss: 5.219578]\n",
      "[Epoch 8/10] [Batch 27/79] [D loss: 0.002866] [G loss: 5.240493]\n",
      "[Epoch 8/10] [Batch 28/79] [D loss: 0.002917] [G loss: 5.240211]\n",
      "[Epoch 8/10] [Batch 29/79] [D loss: 0.002886] [G loss: 5.236191]\n",
      "[Epoch 8/10] [Batch 30/79] [D loss: 0.002895] [G loss: 5.230495]\n",
      "[Epoch 8/10] [Batch 31/79] [D loss: 0.002977] [G loss: 5.223815]\n",
      "[Epoch 8/10] [Batch 32/79] [D loss: 0.003037] [G loss: 5.221357]\n",
      "[Epoch 8/10] [Batch 33/79] [D loss: 0.002978] [G loss: 5.218004]\n",
      "[Epoch 8/10] [Batch 34/79] [D loss: 0.002924] [G loss: 5.242223]\n",
      "[Epoch 8/10] [Batch 35/79] [D loss: 0.002914] [G loss: 5.234871]\n",
      "[Epoch 8/10] [Batch 36/79] [D loss: 0.002902] [G loss: 5.245262]\n",
      "[Epoch 8/10] [Batch 37/79] [D loss: 0.002881] [G loss: 5.246514]\n",
      "[Epoch 8/10] [Batch 38/79] [D loss: 0.002894] [G loss: 5.246956]\n",
      "[Epoch 8/10] [Batch 39/79] [D loss: 0.002849] [G loss: 5.267442]\n",
      "[Epoch 8/10] [Batch 40/79] [D loss: 0.002781] [G loss: 5.277938]\n",
      "[Epoch 8/10] [Batch 41/79] [D loss: 0.002806] [G loss: 5.272636]\n",
      "[Epoch 8/10] [Batch 42/79] [D loss: 0.002767] [G loss: 5.284024]\n",
      "[Epoch 8/10] [Batch 43/79] [D loss: 0.002757] [G loss: 5.295798]\n",
      "[Epoch 8/10] [Batch 44/79] [D loss: 0.002746] [G loss: 5.311546]\n",
      "[Epoch 8/10] [Batch 45/79] [D loss: 0.002766] [G loss: 5.293450]\n",
      "[Epoch 8/10] [Batch 46/79] [D loss: 0.002714] [G loss: 5.311939]\n",
      "[Epoch 8/10] [Batch 47/79] [D loss: 0.002769] [G loss: 5.296538]\n",
      "[Epoch 8/10] [Batch 48/79] [D loss: 0.002691] [G loss: 5.324267]\n",
      "[Epoch 8/10] [Batch 49/79] [D loss: 0.002646] [G loss: 5.327742]\n",
      "[Epoch 8/10] [Batch 50/79] [D loss: 0.002627] [G loss: 5.346004]\n",
      "[Epoch 8/10] [Batch 51/79] [D loss: 0.002580] [G loss: 5.347730]\n",
      "[Epoch 8/10] [Batch 52/79] [D loss: 0.002602] [G loss: 5.347146]\n",
      "[Epoch 8/10] [Batch 53/79] [D loss: 0.002559] [G loss: 5.376727]\n",
      "[Epoch 8/10] [Batch 54/79] [D loss: 0.002557] [G loss: 5.363581]\n",
      "[Epoch 8/10] [Batch 55/79] [D loss: 0.002587] [G loss: 5.370852]\n",
      "[Epoch 8/10] [Batch 56/79] [D loss: 0.002566] [G loss: 5.358326]\n",
      "[Epoch 8/10] [Batch 57/79] [D loss: 0.002548] [G loss: 5.379986]\n",
      "[Epoch 8/10] [Batch 58/79] [D loss: 0.002557] [G loss: 5.369516]\n",
      "[Epoch 8/10] [Batch 59/79] [D loss: 0.002573] [G loss: 5.373701]\n",
      "[Epoch 8/10] [Batch 60/79] [D loss: 0.002521] [G loss: 5.374985]\n",
      "[Epoch 8/10] [Batch 61/79] [D loss: 0.002566] [G loss: 5.362454]\n",
      "[Epoch 8/10] [Batch 62/79] [D loss: 0.002573] [G loss: 5.374957]\n",
      "[Epoch 8/10] [Batch 63/79] [D loss: 0.002518] [G loss: 5.378377]\n",
      "[Epoch 8/10] [Batch 64/79] [D loss: 0.002580] [G loss: 5.357170]\n",
      "[Epoch 8/10] [Batch 65/79] [D loss: 0.002585] [G loss: 5.364855]\n",
      "[Epoch 8/10] [Batch 66/79] [D loss: 0.002532] [G loss: 5.378733]\n",
      "[Epoch 8/10] [Batch 67/79] [D loss: 0.002578] [G loss: 5.356512]\n",
      "[Epoch 8/10] [Batch 68/79] [D loss: 0.002560] [G loss: 5.388732]\n",
      "[Epoch 8/10] [Batch 69/79] [D loss: 0.002571] [G loss: 5.377825]\n",
      "[Epoch 8/10] [Batch 70/79] [D loss: 0.002544] [G loss: 5.379496]\n",
      "[Epoch 8/10] [Batch 71/79] [D loss: 0.002621] [G loss: 5.354830]\n",
      "[Epoch 8/10] [Batch 72/79] [D loss: 0.002559] [G loss: 5.366920]\n",
      "[Epoch 8/10] [Batch 73/79] [D loss: 0.002558] [G loss: 5.375648]\n",
      "[Epoch 8/10] [Batch 74/79] [D loss: 0.002571] [G loss: 5.382944]\n",
      "[Epoch 8/10] [Batch 75/79] [D loss: 0.002577] [G loss: 5.391298]\n",
      "[Epoch 8/10] [Batch 76/79] [D loss: 0.002547] [G loss: 5.384572]\n",
      "[Epoch 8/10] [Batch 77/79] [D loss: 0.002490] [G loss: 5.406887]\n",
      "[Epoch 8/10] [Batch 78/79] [D loss: 0.002566] [G loss: 5.374705]\n",
      "[Epoch 9/10] [Batch 0/79] [D loss: 0.002532] [G loss: 5.395920]\n",
      "[Epoch 9/10] [Batch 1/79] [D loss: 0.002508] [G loss: 5.410361]\n",
      "[Epoch 9/10] [Batch 2/79] [D loss: 0.002536] [G loss: 5.403259]\n",
      "[Epoch 9/10] [Batch 3/79] [D loss: 0.002559] [G loss: 5.395266]\n",
      "[Epoch 9/10] [Batch 4/79] [D loss: 0.002517] [G loss: 5.402605]\n",
      "[Epoch 9/10] [Batch 5/79] [D loss: 0.002596] [G loss: 5.365174]\n",
      "[Epoch 9/10] [Batch 6/79] [D loss: 0.002624] [G loss: 5.367922]\n",
      "[Epoch 9/10] [Batch 7/79] [D loss: 0.002583] [G loss: 5.375114]\n",
      "[Epoch 9/10] [Batch 8/79] [D loss: 0.002593] [G loss: 5.365609]\n",
      "[Epoch 9/10] [Batch 9/79] [D loss: 0.002555] [G loss: 5.393355]\n",
      "[Epoch 9/10] [Batch 10/79] [D loss: 0.002556] [G loss: 5.379965]\n",
      "[Epoch 9/10] [Batch 11/79] [D loss: 0.002507] [G loss: 5.402871]\n",
      "[Epoch 9/10] [Batch 12/79] [D loss: 0.002542] [G loss: 5.399558]\n",
      "[Epoch 9/10] [Batch 13/79] [D loss: 0.002502] [G loss: 5.407782]\n",
      "[Epoch 9/10] [Batch 14/79] [D loss: 0.002470] [G loss: 5.433278]\n",
      "[Epoch 9/10] [Batch 15/79] [D loss: 0.002447] [G loss: 5.430097]\n",
      "[Epoch 9/10] [Batch 16/79] [D loss: 0.002429] [G loss: 5.438422]\n",
      "[Epoch 9/10] [Batch 17/79] [D loss: 0.002500] [G loss: 5.405129]\n",
      "[Epoch 9/10] [Batch 18/79] [D loss: 0.002367] [G loss: 5.452057]\n",
      "[Epoch 9/10] [Batch 19/79] [D loss: 0.002366] [G loss: 5.456494]\n",
      "[Epoch 9/10] [Batch 20/79] [D loss: 0.002446] [G loss: 5.441128]\n",
      "[Epoch 9/10] [Batch 21/79] [D loss: 0.002334] [G loss: 5.479019]\n",
      "[Epoch 9/10] [Batch 22/79] [D loss: 0.002325] [G loss: 5.485764]\n",
      "[Epoch 9/10] [Batch 23/79] [D loss: 0.002282] [G loss: 5.509624]\n",
      "[Epoch 9/10] [Batch 24/79] [D loss: 0.002316] [G loss: 5.500218]\n",
      "[Epoch 9/10] [Batch 25/79] [D loss: 0.002244] [G loss: 5.506355]\n",
      "[Epoch 9/10] [Batch 26/79] [D loss: 0.002256] [G loss: 5.505079]\n",
      "[Epoch 9/10] [Batch 27/79] [D loss: 0.002245] [G loss: 5.504827]\n",
      "[Epoch 9/10] [Batch 28/79] [D loss: 0.002145] [G loss: 5.562310]\n",
      "[Epoch 9/10] [Batch 29/79] [D loss: 0.002132] [G loss: 5.560692]\n",
      "[Epoch 9/10] [Batch 30/79] [D loss: 0.002165] [G loss: 5.554416]\n",
      "[Epoch 9/10] [Batch 31/79] [D loss: 0.002115] [G loss: 5.557879]\n",
      "[Epoch 9/10] [Batch 32/79] [D loss: 0.002130] [G loss: 5.574376]\n",
      "[Epoch 9/10] [Batch 33/79] [D loss: 0.002086] [G loss: 5.579533]\n",
      "[Epoch 9/10] [Batch 34/79] [D loss: 0.002082] [G loss: 5.586703]\n",
      "[Epoch 9/10] [Batch 35/79] [D loss: 0.002081] [G loss: 5.610818]\n",
      "[Epoch 9/10] [Batch 36/79] [D loss: 0.002053] [G loss: 5.598950]\n",
      "[Epoch 9/10] [Batch 37/79] [D loss: 0.002031] [G loss: 5.612735]\n",
      "[Epoch 9/10] [Batch 38/79] [D loss: 0.002046] [G loss: 5.608870]\n",
      "[Epoch 9/10] [Batch 39/79] [D loss: 0.002044] [G loss: 5.600532]\n",
      "[Epoch 9/10] [Batch 40/79] [D loss: 0.002023] [G loss: 5.603765]\n",
      "[Epoch 9/10] [Batch 41/79] [D loss: 0.002077] [G loss: 5.608978]\n",
      "[Epoch 9/10] [Batch 42/79] [D loss: 0.001983] [G loss: 5.624188]\n",
      "[Epoch 9/10] [Batch 43/79] [D loss: 0.002030] [G loss: 5.615407]\n",
      "[Epoch 9/10] [Batch 44/79] [D loss: 0.002005] [G loss: 5.627573]\n",
      "[Epoch 9/10] [Batch 45/79] [D loss: 0.002001] [G loss: 5.633956]\n",
      "[Epoch 9/10] [Batch 46/79] [D loss: 0.002017] [G loss: 5.623923]\n",
      "[Epoch 9/10] [Batch 47/79] [D loss: 0.001997] [G loss: 5.618330]\n",
      "[Epoch 9/10] [Batch 48/79] [D loss: 0.002005] [G loss: 5.615734]\n",
      "[Epoch 9/10] [Batch 49/79] [D loss: 0.002044] [G loss: 5.612350]\n",
      "[Epoch 9/10] [Batch 50/79] [D loss: 0.002003] [G loss: 5.626627]\n",
      "[Epoch 9/10] [Batch 51/79] [D loss: 0.001989] [G loss: 5.621533]\n",
      "[Epoch 9/10] [Batch 52/79] [D loss: 0.002047] [G loss: 5.614973]\n",
      "[Epoch 9/10] [Batch 53/79] [D loss: 0.002035] [G loss: 5.622117]\n",
      "[Epoch 9/10] [Batch 54/79] [D loss: 0.002047] [G loss: 5.611104]\n",
      "[Epoch 9/10] [Batch 55/79] [D loss: 0.002030] [G loss: 5.609785]\n",
      "[Epoch 9/10] [Batch 56/79] [D loss: 0.002045] [G loss: 5.609538]\n",
      "[Epoch 9/10] [Batch 57/79] [D loss: 0.001992] [G loss: 5.618896]\n",
      "[Epoch 9/10] [Batch 58/79] [D loss: 0.002031] [G loss: 5.619376]\n",
      "[Epoch 9/10] [Batch 59/79] [D loss: 0.002054] [G loss: 5.598671]\n",
      "[Epoch 9/10] [Batch 60/79] [D loss: 0.002028] [G loss: 5.600166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/10] [Batch 61/79] [D loss: 0.002049] [G loss: 5.604762]\n",
      "[Epoch 9/10] [Batch 62/79] [D loss: 0.002082] [G loss: 5.593681]\n",
      "[Epoch 9/10] [Batch 63/79] [D loss: 0.002089] [G loss: 5.581590]\n",
      "[Epoch 9/10] [Batch 64/79] [D loss: 0.002115] [G loss: 5.564958]\n",
      "[Epoch 9/10] [Batch 65/79] [D loss: 0.002132] [G loss: 5.567775]\n",
      "[Epoch 9/10] [Batch 66/79] [D loss: 0.002140] [G loss: 5.562744]\n",
      "[Epoch 9/10] [Batch 67/79] [D loss: 0.002141] [G loss: 5.574406]\n",
      "[Epoch 9/10] [Batch 68/79] [D loss: 0.002171] [G loss: 5.548184]\n",
      "[Epoch 9/10] [Batch 69/79] [D loss: 0.002132] [G loss: 5.563546]\n",
      "[Epoch 9/10] [Batch 70/79] [D loss: 0.002136] [G loss: 5.559607]\n",
      "[Epoch 9/10] [Batch 71/79] [D loss: 0.002160] [G loss: 5.545588]\n",
      "[Epoch 9/10] [Batch 72/79] [D loss: 0.002119] [G loss: 5.576668]\n",
      "[Epoch 9/10] [Batch 73/79] [D loss: 0.002079] [G loss: 5.597938]\n",
      "[Epoch 9/10] [Batch 74/79] [D loss: 0.002132] [G loss: 5.582868]\n",
      "[Epoch 9/10] [Batch 75/79] [D loss: 0.002112] [G loss: 5.577043]\n",
      "[Epoch 9/10] [Batch 76/79] [D loss: 0.002124] [G loss: 5.570684]\n",
      "[Epoch 9/10] [Batch 77/79] [D loss: 0.002072] [G loss: 5.600796]\n",
      "[Epoch 9/10] [Batch 78/79] [D loss: 0.001972] [G loss: 5.662092]\n"
     ]
    }
   ],
   "source": [
    "# os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "# parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "# parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "# parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "# parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "# parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "# parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n",
    "# parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "# parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)\n",
    "\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.n_epochs = 10\n",
    "        self.batch_size = 128\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 8\n",
    "        self.latent_dim = 20\n",
    "        self.img_size = 8\n",
    "        self.channels = 1\n",
    "        self.sample_interval = 100\n",
    "\n",
    "opt = Opt()\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "# os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "# dataroot = \"data/raw/raw_8_png\"\n",
    "# dataset = dset.ImageFolder(root=dataroot,\n",
    "#                            transform=transforms.Compose([\n",
    "#                                transforms.Grayscale(),\n",
    "#                                transforms.ToTensor(),\n",
    "#                            ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_imgs.shape)\n",
    "\n",
    "def change_a_value(arr):\n",
    "    arr[2, 2] = 10\n",
    "    return arr\n",
    "\n",
    "imgs = np.array([change_a_value(dataset[i][0]) for i in range(20)]).astype('float')\n",
    "a = discriminator(torch.tensor(imgs, dtype=torch.float))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n",
      "It's a loop : False\n"
     ]
    }
   ],
   "source": [
    "from lib.loop_gen import *\n",
    "\n",
    "N = 8\n",
    "\n",
    "def renormalize(arr):\n",
    "    loop = np.zeros((N, N))\n",
    "    base_elts = arr[0]\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            loop[i, j] = np.argmin(np.abs(arr[i, j] - base_elts))\n",
    "    return loop\n",
    "\n",
    "for tensor in gen_imgs:\n",
    "    arr = tensor.detach().numpy()[0]\n",
    "    loop_arr = renormalize(arr)\n",
    "    my_loop = LoopModel(loop_arr.astype('int'))\n",
    "    print(\"It's a loop :\", my_loop.is_loop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
